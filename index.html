<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="SPPO">
  <meta property="og:title" content="Self-Play Preference Optimization for Language Model Alignment"/>
  <meta property="og:description" content="SPPO is a new self-play algorithm derived from the self-play mechanism to fine-tune large language models efficiently and flexibly."/>
  <meta property="og:url" content="https://github.com/uclaml/SPPO"/>
  

  <meta name="twitter:title" content="Self-Play Preference Optimization for Language Model Alignment">
  <meta name="twitter:description" content="SPPO is a new self-play algorithm derived from the self-play mechanism to fine-tune large language models efficiently and flexibly.">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="LLM, self-play, fine-tuning, RLHF">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Self-Play Preference Optimization for Language Model Alignment</title>
  <link rel="icon" type="image/x-icon" href="static/images/star.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Self-Play Preference Optimization for Language Model Alignment</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->

              <span class="author-block">
                <a href="https://yuewu.us/" target="_blank">Yue Wu</a><sup>1,*</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~zhiqings/" target="_blank">Zhiqing Sun</a><sup>2,*</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=8foZzX4AAAAJ" target="_blank">Huizhuo Yuan</a><sup>1,*</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=FOoKDukAAAAJ" target="_blank">Kaixuan Ji</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~yiming/" target="_blank">Yiming Yang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://web.cs.ucla.edu/~qgu/" target="_blank">Quanquan Gu</a><sup>1</sup>
              </span>
              
              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup> University of California, Los Angeles</span>
                <span class="author-block"><sup>2</sup> Carnegie Mellon University</span>
                <span class="eql-cntrb"><small><br><sup>*</sup> Indicates Equal Contribution</small></span>
              </div>
            
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2405.00675" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/uclaml/SPPO" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2405.00675" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- HuggingFace Model Link -->
              <span class="link-block">
                <a href="https://huggingface.co/collections/UCLA-AGI/sppo-6635fdd844f2b2e4a94d0b9a" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <img src="static/images/hf-logo.svg" alt="My Icon">
                </span>
                <span>Model</span>
              </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed Self-Play Preference Optimization (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys a theoretical convergence guarantee. Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard. Starting from a stronger base model Llama-3-8B-Instruct, we are able to achieve a length-controlled win rate of 38.77%. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- SPPO. -->
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <h2 class="title is-3"> </h2>
    <h2 class="title is-3">Self-Play Preference Optimization (SPPO)</h2>
<!--     <p align="center">
      <img src="images/spin_dalle.png" width="250" height="250"/>
    </p> -->
  </div>
</div>
<!--/ SPPO. -->
<!-- Results. -->
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <div class="content has-text-justified">
        <p>
            Large Language Models (LLMs) such as InstructGPT have demonstrated impressive capabilities, yet they face challenges in ensuring reliability, safety, and ethical alignment. Reinforcement Learning from Human Feedback (RLHF), or Preference-based Reinforcement Learning (PbRL), offers a solution by fine-tuning models to align with human preferences. Traditional RLHF methods rely on reward models to guide this process, but they often fall short of capturing the complexities of human behavior.
        </p>
        <p>
            Recent research highlights the limitations of parametric preference models like Bradley-Terry, which assume consistent and transitive human preferences. Instead, studies suggest that human preferences can be inconsistent and influenced by various factors, challenging the effectiveness of these models.
        </p>
        <p>
            In this paper, we introduce Self-play Probabilistic Preference Optimization (SPPO), a new self-play algorithm designed to solve the two-player constant-sum game for LLM alignment. SPPO utilizes an exponential weight update algorithm within a self-play framework, where policies are fine-tuned on synthetic data generated by the model itself. 
            Our findings suggest that SPPO provides a robust and scalable solution for aligning large language models with human preferences.
        </p>
    </div>
  </div>
</div>
<!--/ Results. -->

<!-- SPPO. -->
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <h2 class="title is-3"> </h2>
    <h2 class="title is-3">Algorithm</h2>
    <p align="center">
      <img src="images/SPPO.png" height="300"/>
    </p>
  </div>
</div>
<!--/ SPPO. -->

<!-- Results. -->
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <h2 class="title is-3">Results</h2>
    <div class="columns is-centered">
      <div class="column is-three-fifths">
        <h2 class="title is-3"> </h2>
        <p align="center">
          <img src="images/table.png" height="300"/>
          <h2 class="subtitle has-text-centered">
            AlpacaEval 2.0 evaluation of models in terms of both normal and length-controlled (LC) win rates in percentage (%).
          </h2>
        </p>
      </div>
    </div>
      <div class="content has-text-justified">
        <p>
          In this study, we adopt Mistral-7B-Instruct-v0.2 and Llama-3-8B-Instruct as our base models. The base models are further fine-tuned by SPPO on the 60k prompts dataset from Ultrafeedback. 
          We use 20k prompts and generate 5 responses per prompt in each iteration. 
          The preferences among the responses are labeled by PairRM, an efficient pair-wise preference model of size 0.4B.
          
          We evaluate SPPO on a wide range of benchmarks, including the HuggingFace Open LLM Leaderboard, MT-Bench, and AlpacaEval 2.0. 
          Our findings highlight several key points: 
        </p>
        <ul>          
          <li> The iterative update via self-play can steadily improve the model performance throughout three iterations, boosting the AlpacaEval2.0 win rate up to 16%.</li>
          <li> SPPO significantly enhances model performance across a wide range of evaluation benchmarks without external supervision from stronger models like GPT-4; </li>
          <li> Compared with other symmetric pairwise losses such as DPO or IPO, the SPPO loss can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response; </li>
        </ul>
    </div>
  </div>
</div>
<!--/ Results. -->
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="images/alpacaeval2.png"/>
        <h2 class="subtitle has-text-centered">
          AlpacaEval 2.0 evaluation of models in terms of both normal and length-controlled (LC) win rates in percentage (%). SPPO demonstrates steady performance gains across iterations and outperforms other baselines which show a tendency to produce longer responses. Additionally, re-ranking with the PairRM reward model (best-of-16) at test time consistently enhances the performance across all models。 SPPO achieves a high win rate <i>without strong external supervision like GPT-4</i>.
        </h2>
       </div>
      <div class="item">
        <!-- Your image here -->
        <img src="images/leaderboard.png"/>
        <h2 class="subtitle has-text-centered">
          Open LLM Leaderboard Evaluation. SPPO fine-tuning improves the base model's performance on different tasks, reaching a state-of-the-art average score of 66.75 for Mistral-7B and 70.29 for Llama-3-8B. For Mistral-7B, subsequent iterations of DPO, IPO, and SPPO see a decline in performance. It is possible that aligning with human preferences (simulated by the PairRM preference model in our study) may not always enhance, and can even detract from overall performance.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="images/density.pdf"/>
        <h2 class="subtitle has-text-centered">
          A pairwise loss like DPO can only enlarge the relative probability gap between the winner and loser. <a href="https://arxiv.org/abs/2402.13228" target="_blank">Pal et al., 2024</a> observed that DPO only drives the loser's likelihood to be small, but the winner's likelihood barely changes.
          We observe the same phenomenon and in contrast, SPPO can boost the probability density of the winner. 
        </h2>
      </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{wu2024self,
        title={Self-play preference optimization for language model alignment},
        author={Wu, Yue and Sun, Zhiqing and Yuan, Huizhuo and Ji, Kaixuan and Yang, Yiming and Gu, Quanquan},
        year={2024}
      }
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
