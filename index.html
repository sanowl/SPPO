<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="SPPO">
  <meta property="og:title" content="Self-Play Preference Optimization for Language Model Alignment"/>
  <meta property="og:description" content="SPPO is a new self-play algorithm derived from the self-play mechanism to fine-tune large language models efficiently and flexibly."/>
  <meta property="og:url" content="https://github.com/uclaml/SPPO"/>
  

  <meta name="twitter:title" content="Self-Play Preference Optimization for Language Model Alignment">
  <meta name="twitter:description" content="SPPO is a new self-play algorithm derived from the self-play mechanism to fine-tune large language models efficiently and flexibly.">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="LLM, self-play, fine-tuning, RLHF">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Self-Play Preference Optimization for Language Model Alignment</title>
  <link rel="icon" type="image/x-icon" href="static/images/star.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Self-Play Preference Optimization for Language Model Alignment</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->

              <span class="author-block">
                <a href="https://yuewu.us/" target="_blank">Yue Wu</a><sup>1,*</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~zhiqings/" target="_blank">Zhiqing Sun</a><sup>2,*</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=8foZzX4AAAAJ" target="_blank">Huizhuo Yuan</a><sup>1,*</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=FOoKDukAAAAJ" target="_blank">Kaixuan Ji</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~yiming/" target="_blank">Yiming Yang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://web.cs.ucla.edu/~qgu/" target="_blank">Quanquan Gu</a><sup>1</sup>
              </span>
              
              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup> University of California, Los Angeles</span>
                <span class="author-block"><sup>2</sup> Carnegie Mellon University</span>
                <span class="eql-cntrb"><small><br><sup>*</sup> Indicates Equal Contribution</small></span>
              </div>
            
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2405.00675" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/uclaml/SPPO" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.01335" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- HuggingFace Model Link -->
              <span class="link-block">
                <a href="https://huggingface.co/collections/UCLA-AGI/sppo-6635fdd844f2b2e4a94d0b9a" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <img src="static/images/hf-logo.svg" alt="My Icon">
                </span>
                <span>Model</span>
              </a>
              </span>

<!--               <!-- HuggingFace Dataset Link -->
              <span class="link-block">
                <a href="https://huggingface.co/collections/UCLA-AGI/datasets-spin-65c3624e98d4b589bbc76f3a" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <img src="static/images/hf-logo.svg" alt="My Icon">
                </span>
                <span>Dataset</span>
              </a>
              </span> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed Self-Play Preference Optimization (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys a theoretical convergence guarantee. Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard. Starting from a stronger base model Llama-3-8B-Instruct, we are able to achieve a length-controlled win rate of 38.77%. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- SPPO. -->
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <h2 class="title is-3"> </h2>
    <h2 class="title is-3">Self-Play Preference Optimization (SPPO)</h2>
    <p align="center">
      <img src="images/spin_dalle.png" width="250" height="250"/>
    </p>
  </div>
</div>
<!--/ SPPO. -->
<!-- Results. -->
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <div class="content has-text-justified">
      <p>
        Our study focuses on fine-tuning LLMs without the need for additional human-annotated 
        data beyond the fine-tuning dataset. Specifically, we consider following question: Can we empower a weak 
        LLM to improve itself without acquiring additional human annotated data? In this paper, we answer this question affirmatively. 
        Inspired by the success of self-play mechanisms in games, we propose
        to convert a weak LLM to a strong one through the lens of self-play, where the model is enhanced
        by playing against itself without requiring any direct supervision. 
        In particular, we propose a novel fine-tuning method called Self-Play fIne-tuNing (SPIN), 
        which begins from a supervised fine-tuned model. 
      </p>
    </div>
  </div>
</div>
<!--/ Results. -->

<!-- SPIN. -->
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <h2 class="title is-3"> </h2>
    <h2 class="title is-3">Algorithm</h2>
    <p align="center">
      <img src="static/images/algorithm.png" height="300"/>
    </p>
  </div>
</div>
<!--/ SPIN. -->

<!-- Results. -->
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <h2 class="title is-3">Results</h2>
    <div class="columns is-centered">
      <div class="column is-three-fifths">
        <h2 class="title is-3"> </h2>
        <p align="center">
          <img src="images/iter_openllm.png" height="300"/>
        </p>
      </div>
    </div>
      <div class="content has-text-justified">
        <p>
          In this study, we adopt zephyr-7b-sft-full as our base model. This model derives from the pre-trained Mistral-7B and has been further fine-tuned
          on the SFT dataset Ultrachat200k1 by HuggingFace. From UltraChat200k, We randomly sample 50k prompts and use the base model to generate the synthetic responses. 
          We evaluate SPIN on a wide range of benchmarks, including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. 
          Our findings highlight several key points: 
        </p>
        <ul>
          <li>SPIN markedly enhances model performance across a wide range of evaluation benchmarks by breaking the limit of SFT; </li>
          <li>even without introducing new human annotated data, SPIN at iteration 0 achieves performance on par to DPO training that utilizes even more data; </li>
          <li>iterative training is a necessary component in SPIN as it breaks the limit of multi-epoch training.</li>
        </ul>
    </div>
  </div>
</div>
<!--/ Results. -->
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/tab.png"/>
        <h2 class="subtitle has-text-centered">
          Test performance of SPIN based on zephyr-7b-sft-full across HuggingFace Open LLM Leaderboard datasets. 
          We also denote the average improvement over last iteration in the Average column.
        </h2>
       </div>
      <div class="item">
        <!-- Your image here -->
        <img src="images/dpo_compare.png"/>
        <h2 class="subtitle has-text-centered">
          Performance comparison with DPO training across the six benchmark datasets. 
          SPIN at iteration 0 achieves comparable performance to DPO training with 62k new data. 
          At iteration 1, SPIN has already surpassed DPO training on the majority of datasets.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/tab2.png"/>
        <h2 class="subtitle has-text-centered">
          Test performance on other reasoning benchmark datasets for SPIN at different iterations 
          and zephyr-7b-sft-full. We report the average score for MT-Bench and the accuracy score for 
          Big Bench datasets under standard few-shot CoT evaluation. On OpenBookQA, we report acc_norm 
          with 1-shot example as similar to previous literature. As similar to Open LLM Leaderboard evaluation, 
          we observe a steady improvement in performance on the other benchmark tasks, with no significant degradation.
        </h2>
      </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Results. -->
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <h2 class="title is-3">Ablation Studies</h2>
      <div class="content has-text-justified">
        <p>
          We examine the effect of synthetic dataset size and training epochs within an iteration. 
          Our analysis demonstrates the effectiveness of the synthetic data used by SPIN compared to 
          the SFT data, as well as the necessity of iterative training in SPIN. 
        </p>
    </div>
  </div>
</div>
<!--/ Results. -->
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/ablation1.png"/>
        <h2 class="subtitle has-text-centered">
          The scaling effect of training size of SPIN compared to SFT on the average score of 
          Open LLM Leaderboard. For SPIN, we consider training data of sizes 14k, 26k and 50k where 
          the larger dataset contains the smaller dataset. The starting point for SPIN (with x-axis 0) is the 
          zephyr-7b-sft-full checkpoint, which has been fine-tuned on Ultrachat200k for 1 epoch. We 
          report the model performance trained for 1 epoch with SPIN on the varying sizes of dataset. We 
          additionally compare with SFT, where we fine-tune Mistral-7B on Ultrachat200k for 3 consecutive 
          epochs and report the model performance at the first epoch as the starting point (with x-axis 0).
        </h2>
       </div>
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/ablation2.png"/>
        <h2 class="subtitle has-text-centered">
          The SPIN training dynamics of zephyr-7b-sft-full on the 50k synthetic data with 
          regard to the number of training epochs during iteration 0. We can observe that iterative training is 
          pivotal as training for more epochs during iteration 0 reaches a limit and cannot surpass iteration 1.
        </h2>
       </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{chen2024selfplay,
        title={Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models}, 
        author={Zixiang Chen and Yihe Deng and Huizhuo Yuan and Kaixuan Ji and Quanquan Gu},
        year={2024},
        eprint={2401.01335},
        archivePrefix={arXiv},
        primaryClass={cs.LG}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
